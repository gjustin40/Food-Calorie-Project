{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# data_path = '../../data/caffe_drinks/'\n",
    "# class_names = os.listdir(data_path)\n",
    "\n",
    "# for name in class_names:\n",
    "#     image_list = glob.glob(data_path + name + '/*')\n",
    "    \n",
    "#     for image in image_list:\n",
    "#         img = Image.open(image)\n",
    "#         img_channel = len(img.split())\n",
    "        \n",
    "#         if img_channel != 3:\n",
    "#             print(image)\n",
    "    \n",
    "#     print(name, 'Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are Using : cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f51b6e1330>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCH = 80\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 4\n",
    "TEST_SIZE = 0.20\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.9\n",
    "FOLD_N = 2\n",
    "CLASS_NUM = 9\n",
    "\n",
    "# GPU 여부\n",
    "if torch.cuda.is_available(): device = torch.device('cuda') \n",
    "else: device = torch.device('cpu')\n",
    "print('We are Using :', device)\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperDataset:\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_func, optimizer, device, dataloader):\n",
    "    total_loss, accuracy = 0, 0\n",
    "    total_samples, correct_samples = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.detach().cpu()\n",
    "        \n",
    "        preds = outputs.argmax(1)\n",
    "        total_samples += preds.size()[0]\n",
    "        correct_samples += preds.eq(labels.view_as(preds)).cpu().sum().item()\n",
    "        \n",
    "    accuracy = (correct_samples / total_samples) * 100\n",
    "    \n",
    "    return total_loss/total_samples, accuracy\n",
    "\n",
    "def test(model, loss_func, device, dataloader):\n",
    "    total_loss, accuracy = 0, 0\n",
    "    total_samples, correct_samples = 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = loss_func(outputs, labels)\n",
    "            total_loss += loss.detach().cpu()\n",
    "\n",
    "            preds = outputs.argmax(1)\n",
    "            total_samples += preds.size()[0]\n",
    "            correct_samples += preds.eq(labels.view_as(preds)).cpu().sum().item()\n",
    "\n",
    "        accuracy = (correct_samples / total_samples) * 100  \n",
    "    \n",
    "    return total_loss/total_samples, accuracy\n",
    "\n",
    "\n",
    "def draw_matrix(model, device, dataloader, class_num):\n",
    "    total_samples, correct_samples = 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    confusion_matrix = torch.zeros(CLASS_NUM, CLASS_NUM)\n",
    "       \n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            preds = outputs.argmax(1)\n",
    "            total_samples += preds.size()[0]\n",
    "            correct_samples += preds.eq(labels.view_as(preds)).cpu().sum().item()\n",
    "                 \n",
    "            # Make Confusion Matrix\n",
    "            for row, col in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[row.long(), col.long()] += 1\n",
    "    \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/caffe_drinks/'\n",
    "dataset = ImageFolder(root=data_path)\n",
    "\n",
    "kfold = KFold(n_splits=FOLD_N, shuffle=True)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch 1 / 80 ---- train_loss: 0.0396 ---- train_acc: 61.9334 ---- test_loss: 0.1790 ---- test_acc: 82.5162\n",
      "Epoch 2 / 80 ---- train_loss: 0.0207 ---- train_acc: 81.1748 ---- test_loss: 0.1361 ---- test_acc: 84.6901\n",
      "Epoch 3 / 80 ---- train_loss: 0.0169 ---- train_acc: 83.9038 ---- test_loss: 0.1220 ---- test_acc: 85.8002\n",
      "Epoch 4 / 80 ---- train_loss: 0.0151 ---- train_acc: 84.8751 ---- test_loss: 0.1133 ---- test_acc: 86.3090\n",
      "Epoch 5 / 80 ---- train_loss: 0.0137 ---- train_acc: 86.2165 ---- test_loss: 0.1215 ---- test_acc: 82.7937\n",
      "Epoch 6 / 80 ---- train_loss: 0.0126 ---- train_acc: 87.0028 ---- test_loss: 0.1053 ---- test_acc: 86.4015\n",
      "Epoch 7 / 80 ---- train_loss: 0.0116 ---- train_acc: 88.1591 ---- test_loss: 0.0944 ---- test_acc: 87.5578\n",
      "Epoch 8 / 80 ---- train_loss: 0.0108 ---- train_acc: 88.9454 ---- test_loss: 0.0915 ---- test_acc: 88.0666\n",
      "Epoch 9 / 80 ---- train_loss: 0.0106 ---- train_acc: 88.8992 ---- test_loss: 0.0929 ---- test_acc: 87.6041\n",
      "Epoch 10 / 80 ---- train_loss: 0.0098 ---- train_acc: 89.9630 ---- test_loss: 0.1006 ---- test_acc: 86.4015\n",
      "Epoch 11 / 80 ---- train_loss: 0.0100 ---- train_acc: 89.7317 ---- test_loss: 0.0956 ---- test_acc: 87.6966\n",
      "Epoch 12 / 80 ---- train_loss: 0.0104 ---- train_acc: 88.5291 ---- test_loss: 0.0945 ---- test_acc: 87.2340\n",
      "Epoch 13 / 80 ---- train_loss: 0.0100 ---- train_acc: 89.5467 ---- test_loss: 0.0917 ---- test_acc: 87.1415\n",
      "Epoch 14 / 80 ---- train_loss: 0.0090 ---- train_acc: 89.7780 ---- test_loss: 0.0887 ---- test_acc: 87.7891\n",
      "Epoch 15 / 80 ---- train_loss: 0.0094 ---- train_acc: 89.9167 ---- test_loss: 0.0874 ---- test_acc: 88.7142\n",
      "Epoch 16 / 80 ---- train_loss: 0.0090 ---- train_acc: 90.6105 ---- test_loss: 0.0901 ---- test_acc: 87.9278\n",
      "Epoch 17 / 80 ---- train_loss: 0.0082 ---- train_acc: 91.0268 ---- test_loss: 0.0940 ---- test_acc: 87.1415\n",
      "Epoch 18 / 80 ---- train_loss: 0.0082 ---- train_acc: 91.1193 ---- test_loss: 0.0842 ---- test_acc: 88.5754\n",
      "Epoch 19 / 80 ---- train_loss: 0.0083 ---- train_acc: 91.6281 ---- test_loss: 0.0841 ---- test_acc: 89.1767\n",
      "Epoch 20 / 80 ---- train_loss: 0.0077 ---- train_acc: 91.4894 ---- test_loss: 0.0957 ---- test_acc: 86.9103\n",
      "Epoch 21 / 80 ---- train_loss: 0.0084 ---- train_acc: 90.8418 ---- test_loss: 0.0919 ---- test_acc: 87.7428\n",
      "Epoch 22 / 80 ---- train_loss: 0.0077 ---- train_acc: 91.9056 ---- test_loss: 0.1034 ---- test_acc: 86.4015\n",
      "Epoch 23 / 80 ---- train_loss: 0.0075 ---- train_acc: 91.9056 ---- test_loss: 0.0974 ---- test_acc: 87.0953\n",
      "Epoch 24 / 80 ---- train_loss: 0.0078 ---- train_acc: 91.9981 ---- test_loss: 0.0984 ---- test_acc: 87.0028\n",
      "Epoch 25 / 80 ---- train_loss: 0.0081 ---- train_acc: 90.5180 ---- test_loss: 0.0855 ---- test_acc: 88.3904\n",
      "Epoch 26 / 80 ---- train_loss: 0.0077 ---- train_acc: 91.2118 ---- test_loss: 0.0856 ---- test_acc: 88.1129\n",
      "Epoch 27 / 80 ---- train_loss: 0.0078 ---- train_acc: 90.7493 ---- test_loss: 0.0840 ---- test_acc: 88.8992\n",
      "Epoch 28 / 80 ---- train_loss: 0.0076 ---- train_acc: 91.2581 ---- test_loss: 0.0815 ---- test_acc: 89.3617\n",
      "Epoch 29 / 80 ---- train_loss: 0.0071 ---- train_acc: 92.4144 ---- test_loss: 0.0827 ---- test_acc: 88.8067\n",
      "Epoch 30 / 80 ---- train_loss: 0.0072 ---- train_acc: 92.2757 ---- test_loss: 0.0907 ---- test_acc: 87.6966\n",
      "Epoch 31 / 80 ---- train_loss: 0.0070 ---- train_acc: 91.7206 ---- test_loss: 0.0945 ---- test_acc: 87.3265\n",
      "Epoch 32 / 80 ---- train_loss: 0.0066 ---- train_acc: 92.6920 ---- test_loss: 0.0842 ---- test_acc: 88.5291\n",
      "Epoch 33 / 80 ---- train_loss: 0.0073 ---- train_acc: 91.3969 ---- test_loss: 0.1075 ---- test_acc: 86.0315\n",
      "Epoch 34 / 80 ---- train_loss: 0.0066 ---- train_acc: 93.2007 ---- test_loss: 0.0978 ---- test_acc: 86.3090\n",
      "Epoch 35 / 80 ---- train_loss: 0.0073 ---- train_acc: 92.1832 ---- test_loss: 0.0921 ---- test_acc: 87.7428\n",
      "Epoch 36 / 80 ---- train_loss: 0.0065 ---- train_acc: 93.3858 ---- test_loss: 0.0934 ---- test_acc: 87.6041\n",
      "Epoch 37 / 80 ---- train_loss: 0.0067 ---- train_acc: 91.8131 ---- test_loss: 0.0853 ---- test_acc: 88.4829\n",
      "Epoch 38 / 80 ---- train_loss: 0.0060 ---- train_acc: 94.0796 ---- test_loss: 0.0862 ---- test_acc: 88.5291\n",
      "Epoch 39 / 80 ---- train_loss: 0.0067 ---- train_acc: 92.5069 ---- test_loss: 0.0875 ---- test_acc: 87.7891\n",
      "Epoch 40 / 80 ---- train_loss: 0.0076 ---- train_acc: 91.5356 ---- test_loss: 0.0969 ---- test_acc: 87.2803\n",
      "Epoch 41 / 80 ---- train_loss: 0.0067 ---- train_acc: 92.6920 ---- test_loss: 0.0800 ---- test_acc: 89.1304\n",
      "Epoch 42 / 80 ---- train_loss: 0.0071 ---- train_acc: 92.5069 ---- test_loss: 0.0915 ---- test_acc: 88.1129\n",
      "Epoch 43 / 80 ---- train_loss: 0.0069 ---- train_acc: 91.6744 ---- test_loss: 0.1035 ---- test_acc: 86.7253\n",
      "Epoch 44 / 80 ---- train_loss: 0.0072 ---- train_acc: 92.1369 ---- test_loss: 0.1035 ---- test_acc: 86.2165\n",
      "Epoch 45 / 80 ---- train_loss: 0.0065 ---- train_acc: 92.5069 ---- test_loss: 0.0944 ---- test_acc: 87.7891\n",
      "Epoch 46 / 80 ---- train_loss: 0.0060 ---- train_acc: 93.4783 ---- test_loss: 0.0922 ---- test_acc: 88.2054\n",
      "Epoch 47 / 80 ---- train_loss: 0.0062 ---- train_acc: 92.8770 ---- test_loss: 0.0809 ---- test_acc: 88.5754\n",
      "Epoch 48 / 80 ---- train_loss: 0.0055 ---- train_acc: 93.8483 ---- test_loss: 0.0826 ---- test_acc: 89.1304\n",
      "Epoch 49 / 80 ---- train_loss: 0.0056 ---- train_acc: 94.0796 ---- test_loss: 0.0886 ---- test_acc: 87.6041\n",
      "Epoch 50 / 80 ---- train_loss: 0.0064 ---- train_acc: 92.3682 ---- test_loss: 0.0873 ---- test_acc: 88.4829\n",
      "Epoch 51 / 80 ---- train_loss: 0.0056 ---- train_acc: 93.6633 ---- test_loss: 0.0906 ---- test_acc: 87.9741\n",
      "Epoch 52 / 80 ---- train_loss: 0.0066 ---- train_acc: 92.9695 ---- test_loss: 0.0834 ---- test_acc: 88.9917\n",
      "Epoch 53 / 80 ---- train_loss: 0.0053 ---- train_acc: 93.9870 ---- test_loss: 0.0836 ---- test_acc: 88.8067\n",
      "Epoch 54 / 80 ---- train_loss: 0.0057 ---- train_acc: 93.5245 ---- test_loss: 0.0828 ---- test_acc: 88.5291\n",
      "Epoch 55 / 80 ---- train_loss: 0.0062 ---- train_acc: 93.0620 ---- test_loss: 0.0815 ---- test_acc: 89.0379\n",
      "Epoch 56 / 80 ---- train_loss: 0.0058 ---- train_acc: 93.7558 ---- test_loss: 0.0919 ---- test_acc: 88.3904\n",
      "Epoch 57 / 80 ---- train_loss: 0.0063 ---- train_acc: 93.4320 ---- test_loss: 0.0881 ---- test_acc: 88.5754\n",
      "Epoch 58 / 80 ---- train_loss: 0.0049 ---- train_acc: 94.4958 ---- test_loss: 0.0937 ---- test_acc: 87.7891\n",
      "Epoch 59 / 80 ---- train_loss: 0.0056 ---- train_acc: 93.9408 ---- test_loss: 0.0902 ---- test_acc: 88.8529\n",
      "Epoch 60 / 80 ---- train_loss: 0.0060 ---- train_acc: 93.1545 ---- test_loss: 0.0900 ---- test_acc: 88.2516\n",
      "Epoch 61 / 80 ---- train_loss: 0.0053 ---- train_acc: 94.1721 ---- test_loss: 0.0860 ---- test_acc: 88.4829\n",
      "Epoch 62 / 80 ---- train_loss: 0.0054 ---- train_acc: 93.8483 ---- test_loss: 0.0882 ---- test_acc: 87.8353\n",
      "Epoch 63 / 80 ---- train_loss: 0.0054 ---- train_acc: 93.6633 ---- test_loss: 0.0869 ---- test_acc: 88.3441\n",
      "Epoch 64 / 80 ---- train_loss: 0.0054 ---- train_acc: 94.3571 ---- test_loss: 0.0918 ---- test_acc: 88.3904\n",
      "Epoch 65 / 80 ---- train_loss: 0.0058 ---- train_acc: 93.1082 ---- test_loss: 0.0904 ---- test_acc: 88.0666\n",
      "Epoch 66 / 80 ---- train_loss: 0.0060 ---- train_acc: 93.2007 ---- test_loss: 0.0842 ---- test_acc: 89.3617\n",
      "Epoch 67 / 80 ---- train_loss: 0.0051 ---- train_acc: 94.0796 ---- test_loss: 0.0866 ---- test_acc: 88.6679\n",
      "Epoch 68 / 80 ---- train_loss: 0.0050 ---- train_acc: 94.4496 ---- test_loss: 0.0884 ---- test_acc: 88.3441\n",
      "Epoch 69 / 80 ---- train_loss: 0.0054 ---- train_acc: 94.4958 ---- test_loss: 0.0868 ---- test_acc: 88.3441\n",
      "Epoch 70 / 80 ---- train_loss: 0.0055 ---- train_acc: 93.7558 ---- test_loss: 0.0885 ---- test_acc: 88.5754\n",
      "Epoch 71 / 80 ---- train_loss: 0.0056 ---- train_acc: 93.2470 ---- test_loss: 0.0872 ---- test_acc: 88.3904\n",
      "Epoch 72 / 80 ---- train_loss: 0.0052 ---- train_acc: 94.2646 ---- test_loss: 0.0839 ---- test_acc: 88.8529\n",
      "Epoch 73 / 80 ---- train_loss: 0.0054 ---- train_acc: 93.4783 ---- test_loss: 0.0979 ---- test_acc: 87.1415\n",
      "Epoch 74 / 80 ---- train_loss: 0.0056 ---- train_acc: 93.7558 ---- test_loss: 0.0887 ---- test_acc: 88.4366\n",
      "Epoch 75 / 80 ---- train_loss: 0.0047 ---- train_acc: 94.7271 ---- test_loss: 0.0833 ---- test_acc: 89.1304\n",
      "Epoch 76 / 80 ---- train_loss: 0.0045 ---- train_acc: 94.9584 ---- test_loss: 0.0847 ---- test_acc: 89.0379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 / 80 ---- train_loss: 0.0053 ---- train_acc: 93.5708 ---- test_loss: 0.0860 ---- test_acc: 89.0842\n",
      "Epoch 78 / 80 ---- train_loss: 0.0055 ---- train_acc: 93.2932 ---- test_loss: 0.0828 ---- test_acc: 89.5005\n",
      "Epoch 79 / 80 ---- train_loss: 0.0046 ---- train_acc: 95.0509 ---- test_loss: 0.0923 ---- test_acc: 88.7142\n",
      "Epoch 80 / 80 ---- train_loss: 0.0054 ---- train_acc: 93.7558 ---- test_loss: 0.0872 ---- test_acc: 89.4542\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch 1 / 80 ---- train_loss: 0.0412 ---- train_acc: 58.8807 ---- test_loss: 0.1970 ---- test_acc: 79.0009\n",
      "Epoch 2 / 80 ---- train_loss: 0.0204 ---- train_acc: 83.0250 ---- test_loss: 0.1312 ---- test_acc: 85.8002\n",
      "Epoch 3 / 80 ---- train_loss: 0.0172 ---- train_acc: 84.0888 ---- test_loss: 0.1138 ---- test_acc: 86.5402\n",
      "Epoch 4 / 80 ---- train_loss: 0.0148 ---- train_acc: 85.9389 ---- test_loss: 0.1051 ---- test_acc: 87.1415\n",
      "Epoch 5 / 80 ---- train_loss: 0.0133 ---- train_acc: 87.4653 ---- test_loss: 0.1025 ---- test_acc: 87.2340\n",
      "Epoch 6 / 80 ---- train_loss: 0.0138 ---- train_acc: 85.5689 ---- test_loss: 0.0950 ---- test_acc: 87.9278\n",
      "Epoch 7 / 80 ---- train_loss: 0.0124 ---- train_acc: 86.5402 ---- test_loss: 0.0960 ---- test_acc: 87.9278\n",
      "Epoch 8 / 80 ---- train_loss: 0.0106 ---- train_acc: 88.8992 ---- test_loss: 0.0937 ---- test_acc: 87.7428\n",
      "Epoch 9 / 80 ---- train_loss: 0.0111 ---- train_acc: 88.2054 ---- test_loss: 0.0969 ---- test_acc: 86.4940\n",
      "Epoch 10 / 80 ---- train_loss: 0.0107 ---- train_acc: 88.8529 ---- test_loss: 0.0950 ---- test_acc: 86.9103\n",
      "Epoch 11 / 80 ---- train_loss: 0.0102 ---- train_acc: 89.6392 ---- test_loss: 0.0909 ---- test_acc: 88.1129\n",
      "Epoch 12 / 80 ---- train_loss: 0.0096 ---- train_acc: 89.6392 ---- test_loss: 0.0911 ---- test_acc: 87.0490\n",
      "Epoch 13 / 80 ---- train_loss: 0.0097 ---- train_acc: 89.1767 ---- test_loss: 0.1039 ---- test_acc: 85.9389\n",
      "Epoch 14 / 80 ---- train_loss: 0.0101 ---- train_acc: 88.7604 ---- test_loss: 0.0925 ---- test_acc: 87.0490\n",
      "Epoch 15 / 80 ---- train_loss: 0.0096 ---- train_acc: 89.7780 ---- test_loss: 0.0861 ---- test_acc: 88.0666\n",
      "Epoch 16 / 80 ---- train_loss: 0.0083 ---- train_acc: 90.2405 ---- test_loss: 0.0890 ---- test_acc: 87.5578\n",
      "Epoch 17 / 80 ---- train_loss: 0.0081 ---- train_acc: 91.7669 ---- test_loss: 0.0894 ---- test_acc: 86.8178\n",
      "Epoch 18 / 80 ---- train_loss: 0.0086 ---- train_acc: 91.2118 ---- test_loss: 0.0933 ---- test_acc: 86.9103\n",
      "Epoch 19 / 80 ---- train_loss: 0.0081 ---- train_acc: 91.4894 ---- test_loss: 0.0927 ---- test_acc: 87.0490\n",
      "Epoch 20 / 80 ---- train_loss: 0.0086 ---- train_acc: 90.6568 ---- test_loss: 0.0888 ---- test_acc: 87.6966\n",
      "Epoch 21 / 80 ---- train_loss: 0.0086 ---- train_acc: 91.0268 ---- test_loss: 0.1039 ---- test_acc: 85.1989\n",
      "Epoch 22 / 80 ---- train_loss: 0.0080 ---- train_acc: 91.4431 ---- test_loss: 0.0944 ---- test_acc: 87.4653\n",
      "Epoch 23 / 80 ---- train_loss: 0.0078 ---- train_acc: 91.7669 ---- test_loss: 0.0938 ---- test_acc: 87.2803\n",
      "Epoch 24 / 80 ---- train_loss: 0.0080 ---- train_acc: 91.0731 ---- test_loss: 0.0879 ---- test_acc: 87.9278\n",
      "Epoch 25 / 80 ---- train_loss: 0.0074 ---- train_acc: 91.9519 ---- test_loss: 0.0825 ---- test_acc: 88.4829\n",
      "Epoch 26 / 80 ---- train_loss: 0.0084 ---- train_acc: 91.4431 ---- test_loss: 0.0934 ---- test_acc: 87.1415\n",
      "Epoch 27 / 80 ---- train_loss: 0.0076 ---- train_acc: 91.1193 ---- test_loss: 0.0884 ---- test_acc: 88.2516\n",
      "Epoch 28 / 80 ---- train_loss: 0.0074 ---- train_acc: 92.3219 ---- test_loss: 0.0901 ---- test_acc: 87.4191\n",
      "Epoch 29 / 80 ---- train_loss: 0.0079 ---- train_acc: 90.7031 ---- test_loss: 0.0937 ---- test_acc: 87.6966\n",
      "Epoch 30 / 80 ---- train_loss: 0.0072 ---- train_acc: 91.5356 ---- test_loss: 0.0941 ---- test_acc: 86.9565\n",
      "Epoch 31 / 80 ---- train_loss: 0.0069 ---- train_acc: 92.2294 ---- test_loss: 0.0835 ---- test_acc: 88.2516\n",
      "Epoch 32 / 80 ---- train_loss: 0.0069 ---- train_acc: 92.7845 ---- test_loss: 0.0883 ---- test_acc: 88.1129\n",
      "Epoch 33 / 80 ---- train_loss: 0.0075 ---- train_acc: 91.2118 ---- test_loss: 0.0904 ---- test_acc: 88.1591\n",
      "Epoch 34 / 80 ---- train_loss: 0.0069 ---- train_acc: 92.1369 ---- test_loss: 0.0888 ---- test_acc: 88.8067\n",
      "Epoch 35 / 80 ---- train_loss: 0.0074 ---- train_acc: 91.9981 ---- test_loss: 0.1110 ---- test_acc: 85.1526\n",
      "Epoch 36 / 80 ---- train_loss: 0.0071 ---- train_acc: 91.9981 ---- test_loss: 0.0908 ---- test_acc: 88.5291\n",
      "Epoch 37 / 80 ---- train_loss: 0.0068 ---- train_acc: 92.6457 ---- test_loss: 0.0906 ---- test_acc: 87.7891\n",
      "Epoch 38 / 80 ---- train_loss: 0.0058 ---- train_acc: 93.8945 ---- test_loss: 0.0866 ---- test_acc: 88.1129\n",
      "Epoch 39 / 80 ---- train_loss: 0.0060 ---- train_acc: 93.3858 ---- test_loss: 0.0946 ---- test_acc: 87.3265\n",
      "Epoch 40 / 80 ---- train_loss: 0.0061 ---- train_acc: 92.7845 ---- test_loss: 0.0967 ---- test_acc: 87.1878\n",
      "Epoch 41 / 80 ---- train_loss: 0.0074 ---- train_acc: 91.6281 ---- test_loss: 0.0887 ---- test_acc: 87.6503\n",
      "Epoch 42 / 80 ---- train_loss: 0.0060 ---- train_acc: 93.0157 ---- test_loss: 0.0869 ---- test_acc: 88.3904\n",
      "Epoch 43 / 80 ---- train_loss: 0.0061 ---- train_acc: 93.3395 ---- test_loss: 0.0891 ---- test_acc: 87.9741\n",
      "Epoch 44 / 80 ---- train_loss: 0.0066 ---- train_acc: 93.0157 ---- test_loss: 0.0858 ---- test_acc: 88.0666\n",
      "Epoch 45 / 80 ---- train_loss: 0.0058 ---- train_acc: 93.3395 ---- test_loss: 0.0905 ---- test_acc: 87.7428\n",
      "Epoch 46 / 80 ---- train_loss: 0.0057 ---- train_acc: 93.5708 ---- test_loss: 0.0905 ---- test_acc: 88.1129\n",
      "Epoch 47 / 80 ---- train_loss: 0.0064 ---- train_acc: 92.5994 ---- test_loss: 0.0944 ---- test_acc: 87.8353\n",
      "Epoch 48 / 80 ---- train_loss: 0.0062 ---- train_acc: 92.4144 ---- test_loss: 0.0877 ---- test_acc: 87.9741\n",
      "Epoch 49 / 80 ---- train_loss: 0.0063 ---- train_acc: 92.8307 ---- test_loss: 0.0931 ---- test_acc: 87.5578\n",
      "Epoch 50 / 80 ---- train_loss: 0.0058 ---- train_acc: 93.4783 ---- test_loss: 0.0874 ---- test_acc: 88.2979\n",
      "Epoch 51 / 80 ---- train_loss: 0.0055 ---- train_acc: 93.5245 ---- test_loss: 0.0915 ---- test_acc: 87.8353\n",
      "Epoch 52 / 80 ---- train_loss: 0.0060 ---- train_acc: 92.7382 ---- test_loss: 0.0850 ---- test_acc: 88.8067\n",
      "Epoch 53 / 80 ---- train_loss: 0.0067 ---- train_acc: 92.6920 ---- test_loss: 0.0933 ---- test_acc: 87.5116\n",
      "Epoch 54 / 80 ---- train_loss: 0.0058 ---- train_acc: 94.1258 ---- test_loss: 0.0871 ---- test_acc: 88.4829\n",
      "Epoch 55 / 80 ---- train_loss: 0.0061 ---- train_acc: 93.4783 ---- test_loss: 0.0985 ---- test_acc: 86.9103\n",
      "Epoch 56 / 80 ---- train_loss: 0.0060 ---- train_acc: 93.3858 ---- test_loss: 0.0864 ---- test_acc: 88.3441\n",
      "Epoch 57 / 80 ---- train_loss: 0.0051 ---- train_acc: 93.9870 ---- test_loss: 0.0914 ---- test_acc: 87.7891\n",
      "Epoch 58 / 80 ---- train_loss: 0.0058 ---- train_acc: 93.1545 ---- test_loss: 0.0997 ---- test_acc: 86.6790\n",
      "Epoch 59 / 80 ---- train_loss: 0.0056 ---- train_acc: 93.9408 ---- test_loss: 0.0943 ---- test_acc: 87.4191\n",
      "Epoch 60 / 80 ---- train_loss: 0.0054 ---- train_acc: 94.0796 ---- test_loss: 0.0883 ---- test_acc: 88.3904\n",
      "Epoch 61 / 80 ---- train_loss: 0.0054 ---- train_acc: 93.8483 ---- test_loss: 0.0906 ---- test_acc: 87.7428\n",
      "Epoch 62 / 80 ---- train_loss: 0.0054 ---- train_acc: 93.7095 ---- test_loss: 0.0924 ---- test_acc: 87.8816\n",
      "Epoch 63 / 80 ---- train_loss: 0.0058 ---- train_acc: 93.2007 ---- test_loss: 0.1028 ---- test_acc: 86.4940\n",
      "Epoch 64 / 80 ---- train_loss: 0.0059 ---- train_acc: 93.0157 ---- test_loss: 0.0917 ---- test_acc: 88.0204\n",
      "Epoch 65 / 80 ---- train_loss: 0.0059 ---- train_acc: 93.0157 ---- test_loss: 0.1017 ---- test_acc: 86.9103\n",
      "Epoch 66 / 80 ---- train_loss: 0.0053 ---- train_acc: 93.6633 ---- test_loss: 0.0935 ---- test_acc: 88.0204\n",
      "Epoch 67 / 80 ---- train_loss: 0.0059 ---- train_acc: 93.5245 ---- test_loss: 0.0851 ---- test_acc: 88.9454\n",
      "Epoch 68 / 80 ---- train_loss: 0.0055 ---- train_acc: 93.3858 ---- test_loss: 0.0946 ---- test_acc: 87.6503\n",
      "Epoch 69 / 80 ---- train_loss: 0.0056 ---- train_acc: 93.6170 ---- test_loss: 0.0960 ---- test_acc: 87.6503\n",
      "Epoch 70 / 80 ---- train_loss: 0.0051 ---- train_acc: 94.5883 ---- test_loss: 0.0892 ---- test_acc: 88.3904\n",
      "Epoch 71 / 80 ---- train_loss: 0.0058 ---- train_acc: 93.3395 ---- test_loss: 0.0955 ---- test_acc: 87.6966\n",
      "Epoch 72 / 80 ---- train_loss: 0.0052 ---- train_acc: 93.8020 ---- test_loss: 0.1030 ---- test_acc: 87.0028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 / 80 ---- train_loss: 0.0059 ---- train_acc: 93.5245 ---- test_loss: 0.0950 ---- test_acc: 87.9278\n",
      "Epoch 74 / 80 ---- train_loss: 0.0057 ---- train_acc: 92.9695 ---- test_loss: 0.0903 ---- test_acc: 88.1129\n",
      "Epoch 75 / 80 ---- train_loss: 0.0049 ---- train_acc: 94.4496 ---- test_loss: 0.0985 ---- test_acc: 87.3265\n",
      "Epoch 76 / 80 ---- train_loss: 0.0054 ---- train_acc: 93.7558 ---- test_loss: 0.0906 ---- test_acc: 88.4366\n",
      "Epoch 77 / 80 ---- train_loss: 0.0045 ---- train_acc: 95.0509 ---- test_loss: 0.0934 ---- test_acc: 87.8816\n",
      "Epoch 78 / 80 ---- train_loss: 0.0051 ---- train_acc: 94.3571 ---- test_loss: 0.0943 ---- test_acc: 87.5116\n",
      "Epoch 79 / 80 ---- train_loss: 0.0053 ---- train_acc: 93.7095 ---- test_loss: 0.0952 ---- test_acc: 87.5578\n",
      "Epoch 80 / 80 ---- train_loss: 0.0051 ---- train_acc: 94.4496 ---- test_loss: 0.0893 ---- test_acc: 87.7891\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
    "    print('--------------------------------')\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    train_subsampler = SubsetRandomSampler(train_idx)\n",
    "    test_subsampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    trainloader = DataLoader(WrapperDataset(dataset,transform=train_transform),\n",
    "                            batch_size=TRAIN_BATCH_SIZE,\n",
    "                            sampler=train_subsampler)                                                                                        \n",
    "    testloader = DataLoader(WrapperDataset(dataset, transform=test_transform),\n",
    "                            batch_size=TEST_BATCH_SIZE,\n",
    "                            sampler=test_subsampler)\n",
    "    \n",
    "    model = torchvision.models.resnet152(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc = nn.Linear(2048, CLASS_NUM)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_loss_per_epoch = []\n",
    "    train_acc_per_epoch = []\n",
    "    test_loss_per_epoch = []\n",
    "    test_acc_per_epoch = []\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, loss_func, optimizer, device, trainloader)\n",
    "        test_loss, test_acc = test(model, loss_func, device, testloader)\n",
    "\n",
    "        train_loss_per_epoch.append(train_loss)\n",
    "        train_acc_per_epoch.append(train_acc)\n",
    "        test_loss_per_epoch.append(test_loss)\n",
    "        test_acc_per_epoch.append(test_acc)\n",
    "        \n",
    "        confusion_matrix = draw_matrix(model, device, testloader, CLASS_NUM)\n",
    "\n",
    "        print('Epoch %d / %d ---- train_loss: %0.4f ---- train_acc: %0.4f ---- test_loss: %0.4f ---- test_acc: %0.4f'\n",
    "             %(epoch+1, EPOCH, train_loss, train_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to Name :  {0: 'americano', 1: 'bubbletea_blacksugar', 2: 'cappuccino', 3: 'caramel_macchiato', 4: 'frappuccino_javachip', 5: 'latte_Strawberry', 6: 'latte_goguma', 7: 'latte_greentea', 8: 'mango_juice'}\n",
      "--------Accuracy of Each Class--------\n",
      "americano : 94.88%\n",
      "bubbletea_blacksugar : 86.63%\n",
      "cappuccino : 81.75%\n",
      "caramel_macchiato : 88.24%\n",
      "frappuccino_javachip : 89.53%\n",
      "latte_Strawberry : 94.83%\n",
      "latte_goguma : 70.97%\n",
      "latte_greentea : 88.02%\n",
      "mango_juice : 96.12%\n"
     ]
    }
   ],
   "source": [
    "label_to_name = {v:k for k,v in dataset.class_to_idx.items()}\n",
    "print('Label to Name : ', label_to_name)\n",
    "\n",
    "confusion_matrix = draw_matrix(model, device, testloader, CLASS_NUM)\n",
    "result = confusion_matrix.diag()/confusion_matrix.sum(1)\n",
    "\n",
    "print('--------Accuracy of Each Class--------')\n",
    "for name, acc in zip(label_to_name.values(), result):\n",
    "    print('%s : %0.2f%%' %(name, acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    train_loss_per_epoch = []\n",
    "    train_acc_per_epoch = []\n",
    "    test_loss_per_epoch = []\n",
    "    test_acc_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (80,) and (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e09964d58ad8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training History\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'id %s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2838\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2839\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2840\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2841\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1741\u001b[0m         \"\"\"\n\u001b[0;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (80,) and (2,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(1,81)\n",
    "y = [train_loss_per_epoch, test_loss_per_epoch]\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"loss and accuracy\")\n",
    "plt.title(\"Training History\")\n",
    "for i in range(len(y[0])):\n",
    "    plt.plot(x,[pt[i] for pt in y],label = 'id %s'%i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
